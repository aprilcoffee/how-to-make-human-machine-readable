{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7d3f45-d6f5-425d-b2eb-1346d50c64f5",
   "metadata": {},
   "source": [
    "# 2 workflow examples for hugginface pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb48076c-6c46-4503-bc25-100bf3834eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import pipeline\n",
    "#from transformers import QuestionAnsweringPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebd879-ca86-4ef4-8f52-2723abc94165",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2022/01/hugging-face-transformers-pipeline-functions-advanced-nlp/\n",
    "\n",
    "# Text Generation\n",
    "\n",
    "https://huggingface.co/tasks/text-generation\n",
    "\n",
    "list of models: https://huggingface.co/models?filter=text-generation\n",
    "\n",
    "different decoding methods: https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "The model will generate the following N characters given a few words or a sentence.\n",
    "\n",
    "We need to initialize the Pipeline with the `‚Äòtext-generation‚Äô` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e6e6a9-3e4c-43ef-bc94-e5b15c1220f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"How many roads must a man walk down to work to get a driver's license or to vote? Well, look at this math. What is the difference when only a tiny fraction of the population is willing to sign up to have a driver's license?\\n\\nHere is a perfect example from the\"}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen_pipeline = pipeline('text-generation', model='gpt2')\n",
    "prompt = 'How many roads must a man walk down'\n",
    "text_gen_pipeline(prompt, max_length=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b06b5-9d58-4282-998f-749cc9c91e8f",
   "metadata": {},
   "source": [
    "By default, it will return a single output of `max_length` provided. \n",
    "\n",
    "However, we can set the `num_return_sequences` parameter to output as many sequences as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e2ac7-db27-44a0-8fc2-70f586614d01",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "https://huggingface.co/tasks/question-answering\n",
    "\n",
    "Given a text(context) and the question, extract the answer.\n",
    "\n",
    "For QnA, we need to initialize the Pipeline with the ‚Äúquestion-answering‚Äù task.\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.16.2/en/task_summary#question-answering\n",
    "\n",
    "https://huggingface.co/docs/transformers/task_summary#extractive-question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6950630d-cb08-4f3c-a581-3e158fd816d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ec7551-91d8-4a8f-8ec3-e14e89029efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6177, start: 34, end: 95\n",
      "Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n"
     ]
    }
   ],
   "source": [
    "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")\n",
    "\n",
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8fd6b1c-ff49-46fb-b12e-852d6914602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46596661-87ce-4c53-b48a-f6903a85c037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many roads must a man walk down, Before you call him a man?\n",
      "Answer: wind\n",
      "Question: How many seas must a white dove sail, Before she sleeps in the sand?\n",
      "Answer: wind\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "The answer, my friend, is blowin' in the wind. The answer is blowin' in the wind.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many roads must a man walk down, Before you call him a man?\",\n",
    "    \"How many seas must a white dove sail, Before she sleeps in the sand?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
