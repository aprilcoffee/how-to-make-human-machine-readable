{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "\n",
    "![markov_chain.jpg](images/markov_chain.jpg)\n",
    "[Source](https://mb-14.github.io/tech/2018/10/24/gomarkov.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "Markov chains are a simple way of **next value prediction**. This value may be for e.g. a next word, the next number or some next decision.\n",
    "\n",
    "All (historic) information used to compute this next value lies in the current value. So the next value is not based on a sequence of previous values, but just on the one current value.\n",
    "\n",
    "(In this implementation this algorithm can't learn anything!)\n",
    "\n",
    "Procedure: \n",
    "- create a dictionary of all words in a given text\n",
    "- store for each word all the direct following words\n",
    "- if a word appears as input: lookup the word in the dictionary and choose one of the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Libraries. '''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize as tok\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 50409 \n",
      "\n",
      "['Aesthetics', 'is', 'a', 'branch', 'of', 'philosophy', 'that', 'deals', 'with', 'the', 'nature', 'of', 'beauty', 'and', 'taste', ',', 'as', 'well', 'as', 'the', 'philosophy', 'of', 'art', '(', 'its', 'own', 'area', 'of', 'philosophy', 'that', 'comes', 'out', 'of', 'aesthetics', ')', '.', 'It', 'examines', 'subjective', 'and', 'sensori-emotional', 'values', ',', 'or', 'sometimes', 'called', 'judgments', 'of', 'sentiment', 'and']\n"
     ]
    }
   ],
   "source": [
    "''' Read text and tokenize it with NLTK. '''\n",
    "\n",
    "# Read a file into the variable text\n",
    "with open('data/wiki_selection.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize text with the function word_tokenize from NLTK\n",
    "token = tok(text)\n",
    "print('Number of tokens:',len(token), '\\n')\n",
    "print(token[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Now we will create our vocabulary in the form of a dictionary: one token as `key` and the preceding token as a possible output (a `value`).\n",
    "\n",
    "In order to do so we will loop through our list of tokens and pair them together. First a little test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aesthetics : is\n",
      "is : a\n",
      "a : branch\n",
      "branch : of\n",
      "of : philosophy\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(token[i], ':', token[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a vocabulary of all tokens and map them to their preceding tokens. '''\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "# Loop through all tokens (except the last one).\n",
    "for i in range(len(token) -1):\n",
    "    # The current token is key\n",
    "    key = token[i]\n",
    "    # The next token is the assigned value.\n",
    "    value = token[i+1]\n",
    "    \n",
    "    # Check if the key is already included into the dictionary.\n",
    "    if key in vocabulary.keys():\n",
    "        # If yes, append the value to this entry.\n",
    "        vocabulary[key].append(value)\n",
    "    else:\n",
    "        # Otherwise create a new entry with the key.\n",
    "        vocabulary[key] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 6945\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All options for\n",
      " Aesthetics : ['is', 'in', ',', 'and', 'is', 'encompasses', 'examines', 'is', ',', '.', 'and']\n"
     ]
    }
   ],
   "source": [
    "''' Inspect all options for a given token. '''\n",
    "key = token[0]\n",
    "print('All options for\\n', key, ':', vocabulary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may see some tokens appear multiple times. This is an easy way to assure that the probabilty for that token is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "The following function takes a text sequence (or just one token) as input.<br>\n",
    "It starts with the last token of the input and looks in the dictionary for all possible next tokens.<br>\n",
    "Then one of this options is picked with the function `np.random.choice()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to generate n next token. '''\n",
    "\n",
    "def generate_text(input_, n_token=1):\n",
    "    # input_ = string of text. Arbitrary length, but only the last token is used for the prediction.\n",
    "    # n_token = number of tokens that the function generates.\n",
    "    \n",
    "    # tokenize input and store it in a list called gentext\n",
    "    gentext = tok(input_)\n",
    "    \n",
    "    # predict n_token new tokens\n",
    "    for i in range(n_token):\n",
    "        # token_inp = last token of gentext\n",
    "        token_inp = gentext[-1]\n",
    "        # check if token is included into the dictionary\n",
    "        if not token_inp in vocabulary.keys():\n",
    "            # pick a random choice if not included\n",
    "            token_inp = random.choice(list(vocabulary.keys()))\n",
    "\n",
    "        # get all options for the last token of gentext\n",
    "        options = vocabulary[token_inp]\n",
    "        # choose one of this options\n",
    "        choice = np.random.choice(options)\n",
    "        # append it to the generated text\n",
    "        gentext.append(choice)\n",
    "    \n",
    "    # create output\n",
    "    output = ''\n",
    "    # loop through all predicted tokens\n",
    "    for token in gentext:\n",
    "        # if token is a punctuation:\n",
    "        if token in string.punctuation:\n",
    "            # append it without a whitespace\n",
    "            output += token\n",
    "        else:\n",
    "            # else add a whitespace before the token\n",
    "            output += ' ' + token\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The artistic code, and thought through science is a `` scientific problems of social\n"
     ]
    }
   ],
   "source": [
    "''' Generated text. '''\n",
    "\n",
    "# Pick a random input\n",
    "key = token[random.randint(0, len(token))]\n",
    "# Or use a string as input\n",
    "key = 'The artistic code' \n",
    "\n",
    "generated_text = generate_text(key, 12)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The artistic code to sexual desirability. In the field of an extremely poor spatial and George A.\n",
      " The artistic code to Herodotus and electronic systems might not tested from each individual practitioners of the aliens\n",
      " The artistic code to be used argument is Sundaram '' even granted, affordable, and communicated.\n",
      " The artistic code to that can understand something not a double meaning that specific experiences of the last\n",
      " The artistic code to which 'beauty' who suggested that understanding of body and pronominals.It is the Madhyamaka\n"
     ]
    }
   ],
   "source": [
    "''' Generate multiple texts at once to see the differences. '''\n",
    "for i in range(5):\n",
    "    print(generate_text(key, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "http://www.cyber-omelette.com/2017/01/markov.html\n",
    "\n",
    "https://mb-14.github.io/tech/2018/10/24/gomarkov.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
