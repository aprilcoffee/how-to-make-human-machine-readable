{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains - n-grams\n",
    "\n",
    "This notebook is based on the [Markov Basic Notebook](https://github.com/experimental-informatics/hands-on-text-generators/blob/master/markov_basic.ipynb).\n",
    "\n",
    "So far we have generated text with a simple vocabulary: it maps a key of one token to a value of one token.\n",
    "\n",
    "Another (maybe better method) is to use n-grams as keys and map them to a value of one token.<br>\n",
    "Then the next token prediction is based on multiple (n) tokens.\n",
    "\n",
    "![ngrams.png](images/ngrams.png)\n",
    "[Source](https://mb-14.github.io/tech/2018/10/24/gomarkov.html)\n",
    "\n",
    "Typical n-grams are of length 2 (bigrams) or 3 (trigrams).<br>\n",
    "For a small dataset trigrams may be too long, because they produce less choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Libraries. '''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize as tok\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Set n-grams. \n",
    "Here we define the variable N_GRAMS. \n",
    "We will use it at different locatinos in our code and it must be always the same. '''\n",
    "N_GRAMS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 50409\n",
      "['Aesthetics', 'is', 'a', 'branch', 'of', 'philosophy', 'that', 'deals', 'with', 'the', 'nature', 'of', 'beauty', 'and', 'taste', ',', 'as', 'well', 'as', 'the', 'philosophy', 'of', 'art', '(', 'its', 'own', 'area', 'of', 'philosophy', 'that', 'comes', 'out', 'of', 'aesthetics', ')', '.', 'It', 'examines', 'subjective', 'and', 'sensori-emotional', 'values', ',', 'or', 'sometimes', 'called', 'judgments', 'of', 'sentiment', 'and']\n"
     ]
    }
   ],
   "source": [
    "''' Read text and tokenize it with NLTK. '''\n",
    "\n",
    "# Read text\n",
    "with open('data/wiki_selection.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "# Tokenize\n",
    "token = tok(text)\n",
    "print('Number of tokens:',len(token))\n",
    "print(token[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Create pairs of tokens: n token as input (`key`) and one token as output (`value`).\n",
    "\n",
    "With the tokenization we have split our text in single tokens.<br>\n",
    "Now we have to put n tokens (as key) together.<br>\n",
    "An easy way is the usage of `' '.join(\"multiple tokens\")`.<br>\n",
    "But with this we will run into trouble with punctuation. For example a key would be<br>\n",
    "\"taste ,\"<br>\n",
    "but it should be<br>\n",
    "\"taste,\".\n",
    "\n",
    "We will use a function found on [stackoverflow](https://stackoverflow.com/a/15950837):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some, tokens\n"
     ]
    }
   ],
   "source": [
    "''' Join tokens with space in between, except it is a punctuation. \n",
    "(We don't need to know how it works. It's enough if we see that it works.) '''\n",
    "\n",
    "def join_punctuation(seq, characters='.,;?!'):\n",
    "    characters = set(characters)\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "    yield current\n",
    "    \n",
    "# Usage:\n",
    "l = ['some', ',', 'tokens']\n",
    "print(' '.join(join_punctuation(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 29811\n"
     ]
    }
   ],
   "source": [
    "''' Create a vocabulary of all tokens and map them to their preceding tokens. '''\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "# Loop through all tokens (except the last n-1 ones).\n",
    "for i in range(len(token) - N_GRAMS -1):\n",
    "    # The current token + N_GRAMS token are key.\n",
    "    key = ' '.join(join_punctuation(token[i:i+N_GRAMS]))\n",
    "    # The next token is the assigned value.\n",
    "    value = token[i+N_GRAMS]\n",
    "    \n",
    "    # Check if the key is already included into the dictionary.\n",
    "    if key in vocabulary.keys():\n",
    "        # If yes, append the value to this entry.\n",
    "        vocabulary[key].append(value)\n",
    "    else:\n",
    "        # Otherwise create a new entry with the key.\n",
    "        vocabulary[key] = [value]\n",
    "        \n",
    "print('Size of the vocabulary:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our vocabulary is much longer than in the basic markov version. This is acutally not so good, because it means that our keys are more specific and less general. In reverse we will have less options for each key in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All options for\n",
      " Aesthetics is : ['a', 'for', 'the']\n"
     ]
    }
   ],
   "source": [
    "''' Inspect all options for a given token. '''\n",
    "key = ' '.join(join_punctuation(token[0:N_GRAMS]))\n",
    "print('All options for\\n', key, ':', vocabulary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_n_grams(input_='', n_token=12, n_grams=1):\n",
    "  \n",
    "    # get random key if input is empty\n",
    "    if input_ == '':\n",
    "        gentext = [random.choice(list(vocabulary.keys()))]\n",
    "        \n",
    "    else:\n",
    "        # tokenize input\n",
    "        gentext = tok(input_)\n",
    "\n",
    "    # predict n_token new tokens\n",
    "    for i in range(n_token):\n",
    "        \n",
    "        # token_inp = last token of gentext\n",
    "        token_inp = ' '.join(join_punctuation(gentext[-n_grams:]))\n",
    "        \n",
    "        # check if token_inp is included into the dictionary\n",
    "        if not token_inp in vocabulary.keys():\n",
    "            # pick a random choice if not included\n",
    "            token_inp = random.choice(list(vocabulary.keys()))\n",
    "            \n",
    "        # get all options for the last token of gentext\n",
    "        options = vocabulary[token_inp]\n",
    "        # choose one of this options\n",
    "        choice = np.random.choice(options)\n",
    "        # append it to the generated text\n",
    "        gentext.append(choice)\n",
    "        \n",
    "    \n",
    "    # when the for-loop is finised, creat the output\n",
    "    output = ''\n",
    "    for token in gentext:\n",
    "        if token in string.punctuation:\n",
    "            output += token\n",
    "        else:\n",
    "            # add a whitespace if token is not a punctuation\n",
    "            output += ' ' + token\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " things as computers as 's is prominently to the field in cognitive science,\n",
      " view emphasizes that in each new piece of data that contains both desirable and\n",
      " unit performs internally(, the nature of reality, is related to this\n"
     ]
    }
   ],
   "source": [
    "''' The function above allows the text genration without text input. '''\n",
    "\n",
    "for i in range(3):\n",
    "    print(generate_text_n_grams(n_grams=N_GRAMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aesthetics is for infants to acquire their first-language?, '' the set of\n",
      " Aesthetics is for the rest of mankind. '' that this will not happen\n",
      " Aesthetics is the only existing substance is mental. The descriptions they gave differed\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_text_n_grams('Aesthetics is', 12, N_GRAMS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further tasks/ experiments\n",
    "\n",
    "- Try it with n_grams = 3\n",
    "- Try it without punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "https://mb-14.github.io/tech/2018/10/24/gomarkov.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
