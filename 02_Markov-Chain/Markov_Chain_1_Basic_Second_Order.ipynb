{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain - Second order text generation\n",
    "\n",
    "# Second Order Character Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first Markov Chain we start with. The probability of a character depends on the last character.\n",
    "\n",
    "For that we will create a dictionary called 'vocabulary'. For each individual token of our text we will store all next tokens.\n",
    "When we generate our text we will pick a random token of this list as we have done it in the first-order text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vocabulary (Training)\n",
    "\n",
    "### Reading a new text and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1161 \n",
      "\n",
      "die geschichtenerzaehler machen weiter die autoind\n"
     ]
    }
   ],
   "source": [
    "# Read a file into the variable text\n",
    "with open('data/alles-macht-weiter.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "import string\n",
    "text=text.replace(\"\\n\",\" \").replace(\"ä\",\"ae\").replace(\"Ä\",\"Ae\").replace(\"ö\",\"oe\").replace(\"Ö\",\"oe\").replace(\"ü\",\"ue\").replace(\"Ü\",\"ue\")\n",
    "text = text.lower()\n",
    "remove_digits = str.maketrans('', '', '0123456789')\n",
    "text = text.translate(remove_digits)\n",
    "text = text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "print('Number of tokens:',len(text), '\\n')\n",
    "print(text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d -> i\n",
      "i -> e\n",
      "e ->  \n",
      "  -> g\n",
      "g -> e\n"
     ]
    }
   ],
   "source": [
    "# show the next character of the current character\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(text[i], \"->\" , text[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "# Loop through all tokens (except the last one).\n",
    "for i in range(len(text) -1):\n",
    "    # The current token is key\n",
    "    key = text[i]\n",
    "    # The next token is the assigned value.\n",
    "    value = text[i+1]\n",
    "    \n",
    "    # Check if the key is already included into the dictionary.\n",
    "    if key in vocabulary.keys():\n",
    "        # If yes, append the value to this entry.\n",
    "        vocabulary[key].append(value)\n",
    "    else:\n",
    "        # Otherwise create a new entry with the key.\n",
    "        vocabulary[key] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a--- ['e', 'c', 'u', 'c', 'r', 'c', 'c', 'e', 'c', 'c', 's', 'p', 'c', 'e', 'c', 'g', 'c', 'c', 'u', 'u', 'u', 'u', 'u', 'u', 'n', 'n', 'c', 'n', 'e', 'e', 'u', 'ß', 's', 'n', 'u', ' ', 'l', 'p', 'u', 'r', 't', 's', 'c', 's', 'd', 'u', 'u', 'u', 'n', 'u', 'u', 'g', 'k', 't', 'n', 'k', 't', 'u', 'e', 'c', 'h', 'c', 'e', 'e', 'c', 'd', 'c', 'e', 'c', 'u', 'l', 'g', 'c', 'l', 'n', 'c', 'u', 'c', 'c', 'u', 'u', 'u', 'p']\n",
      "c--- ['h', 'h', 'h', 'h', 'h', 'h', 'k', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'k', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'k']\n"
     ]
    }
   ],
   "source": [
    "# Show all possible option for specific character\n",
    "print(\"a---\",vocabulary['a'])\n",
    "print(\"c---\",vocabulary['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "### Build a function to pick the value\n",
    "\n",
    "Since this operation is repeatable, we build a function to help picking the next possible character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n"
     ]
    }
   ],
   "source": [
    "''' Return a randomly selected token from our list of options. '''\n",
    "import random\n",
    "\n",
    "def next_token(key):\n",
    "    \n",
    "    # Get all options stored for in the dictionary for this key.\n",
    "    options = vocabulary[key]\n",
    "    \n",
    "    # Pick one.\n",
    "    choice = random.choice(options)\n",
    "    \n",
    "    # Return this value.\n",
    "    return choice\n",
    "\n",
    "print(next_token('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achr weienerochen gen malls maumocht weiteriteien l\n"
     ]
    }
   ],
   "source": [
    "generated_text = 'a' # We start with this as input.\n",
    "\n",
    "# execute 50 times\n",
    "for i in range(50):\n",
    "    \n",
    "    # The last token of generated_text is the key to get the next token.\n",
    "    key = generated_text[-1]\n",
    "    \n",
    "    # Pick one token for this key.\n",
    "    choice = next_token(key)\n",
    "    \n",
    "    # Append this token to the generated text.\n",
    "    generated_text += choice\n",
    "    \n",
    "# We print the generated text once when the for-loop has finished.\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "# Second order Word Generation\n",
    "\n",
    "Since there are more possibility in Word generation, we need a larger data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 43815 \n",
      "\n",
      "['aesthetics', 'is', 'a', 'branch', 'of', 'philosophy', 'that', 'deals', 'with', 'the', 'nature', 'of', 'beauty', 'and', 'taste', 'as', 'well', 'as', 'the', 'philosophy', 'of', 'art', 'its', 'own', 'area', 'of', 'philosophy', 'that', 'comes', 'out', 'of', 'aesthetics', 'it', 'examines', 'subjective', 'and', 'sensoriemotional', 'values', 'or', 'sometimes', 'called', 'judgments', 'of', 'sentiment', 'and', 'tasteaesthetics', 'covers', 'both', 'natural', 'and']\n"
     ]
    }
   ],
   "source": [
    "with open('data/wiki_selection.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "import string\n",
    "text=text.replace(\"\\n\",\" \").replace(\"ä\",\"ae\").replace(\"Ä\",\"Ae\").replace(\"ö\",\"oe\").replace(\"Ö\",\"oe\").replace(\"ü\",\"ue\").replace(\"Ü\",\"ue\")\n",
    "text = text.lower()\n",
    "remove_digits = str.maketrans('', '', '0123456789')\n",
    "text = text.translate(remove_digits)\n",
    "text = text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#With this line we splice the text into lists of words\n",
    "text = text.split()\n",
    "\n",
    "print('Number of tokens:',len(text), '\\n')\n",
    "print(text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'matters', 'separate', 'aesthetics', 'extent', 'thcentury', 'complex', 'of', 'sociologists', 'works', 'delay', 'doubt', 'researchers', 'traditions', 'of', 'debate', 'of', 'such', 'way', 'questions', 'things', 'physical', 'examples', 'insight', 'critics', 'phenomena', 'of', 'familiar', 'hailed', 'to', 'observers', 'branches', 'of', 'class', 'researchers', 'loss', 'statisticians', 'generally', 'similarity', 'of', 'notion', 'measure', 'training', 'nonlinear', 'successful', 'fields', 'social', 'of', 'important', 'of', 'sort', 'thinkers', 'of', 'contemporary', 'extent', 'relevant', 'literary', 'code', 'important', 'of', 'theorists', 'have', 'real', 'collection', 'way', 'borderline', 'given', 'other', 'of', 'of', 'respects', 'physical', 'panpsychists', 'respects', 'underlying', 'other', 'sense', 'philosophers', 'change', 'way', 'philosophers', 'aspect', 'philosophers', 'philosophers', 'of', 'experiential', 'computer', 'brain', 'patients', 'sense', 'take', 'modern', 'philosophers', 'thinkers', 'particular', 'regard', 'versions', 'decades', 'antirealists', 'essential', 'auxiliary', 'oppressive', 'postmodernist', 'such', 'of', 'of', 'examples', 'end', 'philosophers']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "for i in range(len(text) -1):\n",
    "    # The current token is key\n",
    "    key = text[i]\n",
    "    # The next token is the assigned value.\n",
    "    value = text[i+1]\n",
    "    \n",
    "    # Check if the key is already included into the dictionary.\n",
    "    if key in vocabulary.keys():\n",
    "        # If yes, append the value to this entry.\n",
    "        vocabulary[key].append(value)\n",
    "    else:\n",
    "        # Otherwise create a new entry with the key.\n",
    "        vocabulary[key] = [value]\n",
    "        \n",
    "# show possible next value\n",
    "print(vocabulary['some'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paradigm\n"
     ]
    }
   ],
   "source": [
    "# Return a randomly selected token from our list of options. \n",
    "import random\n",
    "\n",
    "def next_token(key):\n",
    "    \n",
    "    # First check if the key is included in the dictionary.\n",
    "    if key not in vocabulary.keys():        \n",
    "        # If not: pick a random key.\n",
    "        key = random.choice(list(vocabulary.keys()))\n",
    "        \n",
    "    # Get all options for this key.\n",
    "    options = vocabulary[key]\n",
    "    \n",
    "    # Return a random choice of this list.\n",
    "    return random.choice(options)\n",
    "\n",
    "print(next_token('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a computer science and information mental representations but electrochemical propertiesa related to modular brain this is not inductive reasoning or less interested in an attempt to deviations and focus on how does not by judea pearl in the self – and verification computer application of physical depending on making the symbolic interactionism is real than astronomy served to the mind is innate and models that if the study of cultural factors play a discipline some works such as islamic philosophers about the next similarly it is represented by a candle the training the indiana philosophy philosophy of experimental histories to strictly\n"
     ]
    }
   ],
   "source": [
    "generated_text = ['a'] # We start with this as input.\n",
    "\n",
    "# execute 50 times\n",
    "for i in range(100):\n",
    "    \n",
    "    ##### The last token of generated_text is the key to get the next token.\n",
    "    #key = generated_text[-1]\n",
    "    \n",
    "    ##### Pick one token for this key.\n",
    "    # choice = next_token(key)\n",
    "    \n",
    "    ##### Append this token to the generated text.\n",
    "    # generated_text += choice\n",
    "    \n",
    "    generated_text.append(next_token(generated_text[-1]))\n",
    "    \n",
    "# We print the generated text once when the for-loop has finished.\n",
    "generated_text = ' '.join(generated_text)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
