{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading / Scraping / Walking through Textcorpora as Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to get data...\n",
    "\n",
    "- [1. read file from your local system](#1)\n",
    "- [2. download Textfiles f.ex. from gutenberg.org](#2)\n",
    "- [2.1. in case your IP is blocked about any reason](#3)\n",
    "- [3. scraping (static) Textcorpora from the Darknet](#4)\n",
    "- [4. scraping (static) Textcorpora from the Web](#5)\n",
    "- [5. scraping PDF's from the Web](#6)\n",
    "- [6. scraping RSS Feeds](#7)\n",
    "- [7. Allison Parrish's Gutenberg Poetry Corpus](#8)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file from local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Geschichtenerzähler machen weiter. die Autoindustrie macht weiter. die Arbeiter machen weiter. d\n"
     ]
    }
   ],
   "source": [
    "#set variable\n",
    "filename = './data/alles-macht-weiter.txt'\n",
    "# open file\n",
    "file = open(filename, 'rt')\n",
    "#read it in\n",
    "amw1 = file.read()\n",
    "#close it\n",
    "file.close()\n",
    "#print the first 101 items\n",
    "print(amw1[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### download Textfiles f.ex. from gutenberg.org (Schuld und Sühne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n",
      "\t  \"http://www.w3.org/TR/REC-html40/loose.dtd\">\n",
      "\n",
      "<html lang=\"en\" dir=\"ltr\">\n",
      "  <HEAD>\n",
      "    <META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n",
      "    <TITLE>German addresses are blocked - www.gutenberg.org\n",
      "    </TITLE>\n",
      "    <meta charset=\"UTF-8\" />\n",
      "    <link rel=\"shortcut icon\" href=\"/favicon.ico\" />\n",
      "  </HEAD>\n",
      "  <body>\n",
      "\n",
      "<font color=\"red\">    \n",
      "  <h1 align=\"center\">Your IP Address in Germany is Blocked from www.gutenberg.org</h1>\n",
      "</font>\n",
      "\n",
      "  <p>We a\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import requests\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "\n",
    "#request the text\n",
    "r = requests.get(url)\n",
    "\n",
    "#print the first 527 characters\n",
    "print(r.text[0:527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### in case your IP is blocked about any reason\n",
    "\n",
    "you can scrape f.ex. over the TOR-SOCKS Proxy like that (you have to have installed TOR first on your machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request: {\n",
      "  \"origin\": \"93.132.181.161\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"origin\": \"93.132.181.161\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"origin\": \"95.211.230.211\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import requests\n",
    "\n",
    "#get your usual IP adress\n",
    "r = requests.get('http://httpbin.org/ip')\n",
    "print(\"request:\", r.text)\n",
    "\n",
    "#creating now an empty session object\n",
    "session = requests.session()\n",
    "session.proxies = {}\n",
    "\n",
    "#get your usual IP adress\n",
    "s = session.get('http://httpbin.org/ip')\n",
    "print(s.text)\n",
    "\n",
    "#adding TOR proxy\n",
    "session.proxies['http'] = 'socks5h://localhost:9050'\n",
    "session.proxies['https'] = 'socks5h://localhost:9050'\n",
    "\n",
    "#get the new IP adress\n",
    "t = session.get('http://httpbin.org/ip')\n",
    "print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Crime and Punishment\n",
      "\n",
      "Author: Fyodor Dostoevsky\n",
      "\n",
      "Release Date: March 28, 2006 [EBook #2554]\n",
      "Last Updated: October 27, 2016\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now get the data you'd like to scrape\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "t2 = session.get(url)\n",
    "print(t2.text[0:527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###  if you wanna scrape Textcorpora like that (in HTML-Format) from the Darknet\n",
    "\n",
    "* List of Librairies in the darknet: http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/Libraries\n",
    "* The Hidden Wiki: http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/index.php/Main_Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLEM XIV\n",
      "FOREWORD BY IRVING T. CREVE, M.A., PH.D.\n",
      "INTRODUCTION BY THOMAS B. FULLER II, GENERAL, U.S. ARMY, RET.\n",
      "AFTERWORD BY RICHARD POPP\n",
      "INDIANA UNIVERSITY PRESS\n",
      "2047\n",
      "Foreword\n",
      "To pinpoint the moment in history when the abacus acquired reason is as difficult as saying exactly when the ape turned into man. And yet barely one human life span has lapsed since the moment when, with the construction of Vannevar Bush's differential-equation analyzer, intellectronics began its turbulent development, eniac, which followed toward the close of World War II, was the machine that gave riseâ prematurely, of courseâto the name\"electronic brain.\" eniac was in fact a computer and, when measured on the tree of life, a primitive nerve ganglion. Yet historians date the age of computerization from it. In the 1950s a considerable demand for calculating machines developed. One of the first concerns to put them into mass production was IBM.\n",
      "Those devices had little in common with the processes of thought. They were used as data processors in the field of economics and by big business, as well as in administration and science. They also entered politics: the earliest were used to predict the results of Presidential elections. At more or less the same time the RAND Corporation began to interest military circles at the Pentagon in a method of predicting occurrences in the international politico-military arena, a method relying on the formulation of so-called \"scenarios of events.\" From there it was only a short distance to\n"
     ]
    }
   ],
   "source": [
    "# replace the WebURL with an Onion-Adress\n",
    "dw = session.get('http://libraryqtlpitkix.onion/library/Fiction/Stanislaw%20Lem%20-%20GOLEM%20XIV.txt')\n",
    "#print(dw.headers, \"\\n\")\n",
    "print(dw.text[0:1527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Download (static) Textcorpora as (HTML) from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:my=\"mynames\" lang=\"de\"><!-- DEBUG start 14:41:53+01:00 page_id=4627 :: Netzökonomie--><!--\\n\\t\\tCo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"https://taz.de/Vorwuerfe-von-schwarzer-KI-Forscherin/!5730475/\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "Vorwürfe von schwarzer KI-Forscherin: Proteste bei Google\n",
      "\n",
      "Die bekannte KI-Forscherin Timnit Gebru verlässt Google im Streit. Grund ist eine Studie zu Sprachverarbeitung, die dem Konzern nicht passt.\n",
      "Forscherin Timnit Gebru wirft ihrem Ex-Arbeitgeber Zensur vor  Foto: Kristin Callahan/ZUMA Press/imago\n",
      "Google liebt sein Image als uneigennütziger Tech-Konzern. Da passt es nicht gut ins Bild, wenn Tausende Mitarbeiter:innen protestieren und in einem offenen \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "print(type(raw))\n",
    "print(raw[440:900])\n",
    "#tokens = nltk.word_tokenize(raw)\n",
    "#print(tokens[300:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### download PDF's from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set URL\n",
    "url = \"https://www.christian-lindner.de/reden\"\n",
    "\n",
    "#If there is no such folder, create one automatically\n",
    "folder_location = r'./lindner-talks'\n",
    "if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "#get all pdf's on this page and store it into the folder\n",
    "response = requests.get(url)\n",
    "soup= BeautifulSoup(response.text, \"html.parser\")     \n",
    "for link in soup.select(\"a[href$='.pdf']\"):\n",
    "    #Name the pdf files using the last portion of each link which are unique in this case\n",
    "    filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(requests.get(urljoin(url,link['href'])).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#define the page to parse\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "#define what you want to see (the title of the feed-page):\n",
    "llog['feed']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how much posts the page have?\n",
    "len(llog.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Translate Sabotage, part 2'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set a variable for the 3 post\n",
    "post = llog.entries[2]\n",
    "#print the title from it\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>This is all over the Chinese internet:</p>\\n<p align=\"center\"><a hre'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set variable for its content\n",
    "content = post.content[0].value\n",
    "#print the first 71 items\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is all over the Chinese internet:\n",
      " (source)\n",
      "\n",
      "The Chinese translation given is:\n",
      "Zhōngguó xìnshǒu nuòyán\n",
      "中国信守诺言\n",
      "\"China keeps its promise\"\n",
      "That's the exact opposite of the English original:  \"China breaks promise\"\n",
      "This is so far off and politically motivated that it seems someone, or some group, must have sabotaged Google Translate in this particular case.\n",
      "Baidu Fanyi correctly renders \"China breaks promise\" as \" Zhōngguó shīxìn le 中国失信了\".  It's an easy sentence.\n",
      "Update: Google has already corrected it. The translation of \"China breaks promise\" is now correctly rendered as \" Zhōngguó wéi nuò 中国违诺\".\n",
      "Selected readings\n",
      "\n",
      "\"A Japanese-French Google Translate mixup\" (7/13/20)\n",
      "\"More Google Translate hallucinations on YouTube\" (6/3/18)\n",
      "\"Google Translate sabotage\" (6/14/19)\n",
      "\"The elegance of Google Translate\" (3/10/18)\n",
      "\"The wonders of Google Translate\" (9/22/17)\n",
      "\"Don't blame Google Translate\" (2/4/18)\n",
      "\"Google Translate is even better now\" (9/27/16)\n",
      "\"Google is scary good\" (7/31/17)\n",
      "\"Can't find on Google\" (8/12/14)\n",
      "\n",
      "[Thanks to Chau Wu]\n"
     ]
    }
   ],
   "source": [
    "#parse the html content with beautifulsoup as text\n",
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "#print it\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Allison Parrish's Gutenberg Poetry Corpus\n",
    "see: https://github.com/aparrish/gutenberg-poetry-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By [Allison Parrish](https://www.decontextualize.com/)\n",
    "\n",
    "Allison Parrish made a corpus of around three million lines of poetry from Project Gutenberg. In her notebook [A Project Gutenberg Poetry Corpus: Quick Experiments](https://github.com/aparrish/gutenberg-poetry-corpus/blob/master/quick-experiments.ipynb) she shows a couple of quick examples and experiments in using the corpus in Python. the following examples are from this notebook:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the corpus via this [link](http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz) and store it in the same folder then this notebook is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is in gzipped [newline delimited JSON format](http://ndjson.org/): there's a JSON object on each line. You don't need to decompress the file to work with it, since Python has a handy library for working with gzipped files right in the code. The following cell will read in the file and create a list `all_lines` that contains all of these JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 52.2M  100 52.2M    0     0   866k      0  0:01:01  0:01:01 --:--:-- 2203k\n"
     ]
    }
   ],
   "source": [
    "# download it via `curl`\n",
    "!curl -O http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip it\n",
    "import gzip, json\n",
    "all_lines = []\n",
    "for line in gzip.open(\"gutenberg-poetry-v001.ndjson.gz\"):\n",
    "    all_lines.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s': 'Therefore will I advise not / the struggle to begin.', 'gid': '7321'},\n",
       " {'s': 'Wherof he couthe a gret partie,', 'gid': '266'},\n",
       " {'s': 'More than their houses or lands,', 'gid': '38927'},\n",
       " {'s': 'Some perfect thought of thine.', 'gid': '28591'},\n",
       " {'s': 'Head, shoulders, chest, or waist, I little reck;', 'gid': '14410'},\n",
       " {'s': 'I had a mother once, and her dear name', 'gid': '7122'},\n",
       " {'s': 'Had been her pathway to the sea, and still', 'gid': '29700'},\n",
       " {'s': 'To some addition; that thou well, mayst mark', 'gid': '8796'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract randomly lines of it\n",
    "import random\n",
    "random.sample(all_lines, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object has a key `s` that contains the text of the line of poetry, and a key `gid` that contains the Project Gutenberg ID of the file in question. You can use this ID to look up the title and author of the book of poetry that the line came from (either using the [Project Gutenberg website](https://www.gutenberg.org/) or using pre-built metadata from, e.g., [Gutenberg, dammit](https://github.com/aparrish/gutenberg-dammit/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'s': '“O where dost thou trip it,” the patriarch said,', 'gid': '30185'}, {'s': 'So she strove against her weakness,', 'gid': '16436'}, {'s': 'Gleams of a happier world for younger men,', 'gid': '30599'}, {'s': 'For anything but comradeship', 'gid': '37852'}, {'s': 'That I could gladly plunge to seek', 'gid': '8187'}, {'s': 'There now did Dido, Sidon-born, uprear a mighty fane', 'gid': '29358'}, {'s': \"And the Heaven's sunny gleam,\", 'gid': '2303'}, {'s': \"The Master's hopes and realize his dream.\", 'gid': '8820'}]\n",
      "～ ❀ ～\n",
      "['“O where dost thou trip it,” the patriarch said,', 'So she strove against her weakness,', 'Gleams of a happier world for younger men,', 'For anything but comradeship', 'That I could gladly plunge to seek', 'There now did Dido, Sidon-born, uprear a mighty fane', \"And the Heaven's sunny gleam,\", \"The Master's hopes and realize his dream.\"]\n",
      "～ ❀ ～\n",
      "“O where dost thou trip it,” the patriarch said,\n",
      "So she strove against her weakness,\n",
      "Gleams of a happier world for younger men,\n",
      "For anything but comradeship\n",
      "That I could gladly plunge to seek\n",
      "There now did Dido, Sidon-born, uprear a mighty fane\n",
      "And the Heaven's sunny gleam,\n",
      "The Master's hopes and realize his dream.\n"
     ]
    }
   ],
   "source": [
    "randompoem = random.sample(all_lines, 8)\n",
    "print(randompoem)\n",
    "print(\"～ ❀ ～\")\n",
    "\n",
    "randompoem_t = [line['s'] for line in randompoem]\n",
    "print(randompoem_t)\n",
    "print(\"～ ❀ ～\")\n",
    "\n",
    "randompoem_lb = \"\\n\".join(randompoem_t)\n",
    "print(randompoem_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "you could also f.ex. find in our random output a specific word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(206, 210), match='Dido'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dido = re.search('Dido', randompoem_lb)\n",
    "print(dido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There bowl in hand stands Dido forth, most excellently fair,',\n",
       " \"Then shalt thou call on injur'd Dido's name:\",\n",
       " 'Thrice Dido tried to raise her drooping head,',\n",
       " \"But furious Dido, with dark thoughts involv'd,\",\n",
       " 'Upon a horse of Sidon came, whom that bright Dido gave',\n",
       " ' _The next_: Dido, perhaps not named by Virgil because to him she',\n",
       " \"Queen Dido's gift, and of the Tyrian breed.\",\n",
       " 'Sidonian Dido wrought for him, and, glad the toil to bear,']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or finding a specific word in the whole document an printing the whole line (or 8 of it) \n",
    "dido_line = [line['s'] for line in all_lines if re.search('Dido', line['s'])]\n",
    "random.sample(dido_line, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
