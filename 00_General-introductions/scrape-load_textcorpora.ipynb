{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading / Scraping / Walking through Textcorpora as Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to get data...\n",
    "\n",
    "- [1. read file from your local system](#1)\n",
    "- [2. download Textfiles f.ex. from gutenberg.org](#2)\n",
    "- [2.1. in case your IP is blocked about any reason](#3)\n",
    "- [3. scraping (static) Textcorpora from the Darknet](#4)\n",
    "- [4. scraping (static) Textcorpora from the Web](#5)\n",
    "- [5. scraping PDF's from the Web](#6)\n",
    "- [6. scraping RSS Feeds](#7)\n",
    "- [7. Allison Parrish's Gutenberg Poetry Corpus](#8)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file from local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Geschichtenerzähler machen weiter. die Autoindustrie macht weiter. die Arbeiter machen weiter. d\n"
     ]
    }
   ],
   "source": [
    "#set variable\n",
    "filename = './data/alles-macht-weiter.txt'\n",
    "# open file\n",
    "file = open(filename, 'rt')\n",
    "#read it in\n",
    "amw1 = file.read()\n",
    "#close it\n",
    "file.close()\n",
    "#print the first 101 items\n",
    "print(amw1[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### download Textfiles f.ex. from gutenberg.org (Schuld und Sühne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import requests\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "\n",
    "#request the text\n",
    "r = requests.get(url)\n",
    "\n",
    "#print the first 527 characters\n",
    "print(r.text[0:527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### in case your IP is blocked about any reason\n",
    "\n",
    "you can scrape f.ex. over the TOR-SOCKS Proxy like that (you have to have installed TOR first on your machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import requests\n",
    "\n",
    "#get your usual IP adress\n",
    "r = requests.get('http://httpbin.org/ip')\n",
    "print(\"request:\", r.text)\n",
    "\n",
    "#creating now an empty session object\n",
    "session = requests.session()\n",
    "session.proxies = {}\n",
    "\n",
    "#get your usual IP adress\n",
    "s = session.get('http://httpbin.org/ip')\n",
    "print(s.text)\n",
    "\n",
    "#adding TOR proxy\n",
    "session.proxies['http'] = 'socks5h://localhost:9050'\n",
    "session.proxies['https'] = 'socks5h://localhost:9050'\n",
    "\n",
    "#get the new IP adress\n",
    "t = session.get('http://httpbin.org/ip')\n",
    "print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get the data you'd like to scrape\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "t2 = session.get(url)\n",
    "print(t2.text[0:527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###  if you wanna scrape Textcorpora like that (in HTML-Format) from the Darknet\n",
    "\n",
    "* List of Librairies in the darknet: http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/Libraries\n",
    "* The Hidden Wiki: http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/index.php/Main_Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the WebURL with an Onion-Adress\n",
    "dw = session.get('http://libraryqtlpitkix.onion/library/Fiction/Stanislaw%20Lem%20-%20GOLEM%20XIV.txt')\n",
    "#print(dw.headers, \"\\n\")\n",
    "print(dw.text[0:1527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Download (static) Textcorpora as (HTML) from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:my=\"mynames\" lang=\"de\"><!-- DEBUG start 15:36:29+01:00 page_id=4627 :: Netzökonomie--><!--\\n\\t\\tCo'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"https://taz.de/Vorwuerfe-von-schwarzer-KI-Forscherin/!5730475/\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Vorwürfe von schwarzer KI-Forscherin: Proteste bei Google\n",
      "\n",
      "Die bekannte KI-Forscherin Timnit Gebru verlässt Google im Streit. Grund ist eine Studie zu Sprachverarbeitung, die dem Konzern nicht passt.\n",
      "Forscherin Timnit Gebru wirft ihrem Ex-Arbeitgeber Zensur vor  Foto: Kristin Callahan/ZUMA Press/imago\n",
      "Google liebt sein Image als uneigennütziger Tech-Konzern. Da passt es nicht gut ins Bild, wenn Tausende Mitarbeiter:innen protestieren und in einem offenen Bri\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "print(type(raw))\n",
    "print(raw[438:900])\n",
    "#tokens = nltk.word_tokenize(raw)\n",
    "#print(tokens[300:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### download PDF's from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set URL\n",
    "url = \"https://www.christian-lindner.de/reden\"\n",
    "\n",
    "#If there is no such folder, create one automatically\n",
    "folder_location = r'./data/lindner-talks'\n",
    "if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "#get all pdf's on this page and store it into the folder\n",
    "response = requests.get(url)\n",
    "soup= BeautifulSoup(response.text, \"html.parser\")     \n",
    "for link in soup.select(\"a[href$='.pdf']\"):\n",
    "    #Name the pdf files using the last portion of each link which are unique in this case\n",
    "    filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(requests.get(urljoin(url,link['href'])).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install feedparser\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#define the page to parse\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "#define what you want to see (the title of the feed-page):\n",
    "llog['feed']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how much posts the page have?\n",
    "len(llog.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Naxi writing'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set a variable for the 3 post\n",
    "post = llog.entries[2]\n",
    "#print the title from it\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>From S. Robert Ramsey:</p>\\n<p align=\"center\"><a href=\"http://langua'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set variable for its content\n",
    "content = post.content[0].value\n",
    "#print the first 71 items\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From S. Robert Ramsey:\n",
      "\n",
      "The Naxi Story of Creation and the Great Flood\n",
      "There’s no such thing as pictographic writing. – Or is there?\n",
      "The Naxi, a national minority indigenous to China’s extreme southwest, have what looks for all the world like pictographic writing as its literary tradition. Shown above is a reproduction of one of the most important texts in the Naxi canon, the story of Creation and the subsequent Great Flood.\n",
      "The central theme of the text is that the disastrous flood was brought about by incest between the first humans. This unclean act, incest, had been strictly proscribed by the gods:\n",
      "[The Great God of Light said to them:]\n",
      "“Five Rii Brothers: Brothers must not fight among themselves.” [Panel 1]\n",
      "“Six Chimi Sisters: Sisters must not quarrel.” [Panel 2]\n",
      "“Brothers and Sisters: You must not marry each other. [Panel 3]\n",
      "“For if you do, unclean things will come forth from the sky, from the earth.., from the sun, from the moon… [etc., etc.] Above there will be violent landslides and flooding. Down below, the rising waters will suddenly block the valleys and turn each one into a quagmire.”\n",
      "It’s true that the symbols here are obviously drawings of real-world objects, at least for the most part. The drawings are grouped inside rectangular frames arranged comic-book style on the page and the frames “read” sequentially from left to right–but not necessarily. There’s much backtracking and movement up and down, and sometimes the same drawing is referred to several times as the story unfolds.\n",
      "But the main thing is, the symbols aren’t actually “read.” They’re mnemonic devices used by a priest of the Bon religion to remind him of the details of a story he already knows by heart. Many of the words, especially abstract concepts, are left completely unrepresented. Sometimes a drawing is “read” two or three times even if it only appears once.\n",
      "This is not writing as we know it. The Naxi have never used these pictographs to communicate with each other. They do not exchange messages, write books, or even keep simple records with them. Only someone versed in the mystical lore of the Naxi religion can interpret its meaning. It’s not enough simply to be able to speak Naxi.\n",
      "And yet, the pictographs are almost writing. A drawing of eyes can mean “fate” in Naxi, because the Naxi words for ‘eyes’ and ‘fate’ sound alike. That’s the kind of sound association that, long ago, made writing possible for ancient civilizations, from Mesopotamia to Egypt and to China–and even for Mayan civilization, and the Naxi seem also to have discovered that critical principle. It’s that aspect of the Naxi tradition that scholars of language and writing find so interesting.\n",
      "VHM:  The Nakhi / Naxi were studied for two decades by the Austrian-American explorer and botanist, Joseph Rock.  The language of the Nakhi can be written with the Geba syllabary, but also with the symbols referred to as Dongba.  Rock collected many texts in Dongba and also compiled a dictionary for the script.  Dongba symbols are supposedly pictographic-ideographic, but I doubt that they are a full writing system.  Instead, I suspect that they are probably prompts for priests, but do not directly transcribe spoken Nakhi language.\n",
      "I wrote those words eight years ago (here) and am pleased they are in full agreement with what Bob Ramsey says above in this post.\n",
      "Notes on Naxi language\n",
      "Naxi (Naqxi IPA: [nɑ˨˩ ɕi˧˧]), also known as Nakhi, Nasi, Lomi, Moso, Mo-su, is a Sino-Tibetan language or group of languages spoken by some 310,000 people, most of whom live in or around Lijiang City Yulong Naxi Autonomous County of the province of Yunnan, China. Nakhi is also the ethnic group that speaks it, although in detail, officially defined ethnicity and linguistic reality do not coincide neatly: there are speakers of Naxi who are not registered as \"Naxi\" and citizens who are officially \"Naxi\" but do not speak it.\n",
      "It is commonly proposed in Chinese scholarship that the Naic languages are Lolo-Burmese languages: for instance, Ziwo Lama (2012) classifies Naxi as part of a \"Naxish\" branch of Loloish.\n",
      "However, as early as 1975, Sino-Tibetan linguist David Bradley pointed out that Naxi does not partake in the shared innovations that define Loloish. Thurgood and La Polla (2003) state that \"The position of Naxi … is still unclear despite much speculation\" and leave it unclassified within Sino-Tibetan. Guillaume Jacques & Alexis Michaud (2011) classify Naxi within the Naish lower-level subgroup of Sino-Tibetan; in turn, Naish is part of Naic, itself part of a proposed \"Na-Qiangic\" branch.\n",
      "(source)\n",
      "Selected readings\n",
      "\n",
      "\"Miao / Hmong\" (1/13/22)\n",
      "\"The cattle-keeping Bai of Yunnan\" (1/18/22)\n",
      "\"No word for father\" (10/22/14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#parse the html content with beautifulsoup as text\n",
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "#print it\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Allison Parrish's Gutenberg Poetry Corpus\n",
    "see: https://github.com/aparrish/gutenberg-poetry-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By [Allison Parrish](https://www.decontextualize.com/)\n",
    "\n",
    "Allison Parrish made a corpus of around three million lines of poetry from Project Gutenberg. In her notebook [A Project Gutenberg Poetry Corpus: Quick Experiments](https://github.com/aparrish/gutenberg-poetry-corpus/blob/master/quick-experiments.ipynb) she shows a couple of quick examples and experiments in using the corpus in Python. the following examples are from this notebook:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the corpus via this [link](http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz) and store it in the same folder then this notebook is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is in gzipped [newline delimited JSON format](http://ndjson.org/): there's a JSON object on each line. You don't need to decompress the file to work with it, since Python has a handy library for working with gzipped files right in the code. The following cell will read in the file and create a list `all_lines` that contains all of these JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download it via `curl`\n",
    "!curl -O http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip it\n",
    "import gzip, json\n",
    "all_lines = []\n",
    "for line in gzip.open(\"gutenberg-poetry-v001.ndjson.gz\"):\n",
    "    all_lines.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract randomly lines of it\n",
    "import random\n",
    "random.sample(all_lines, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object has a key `s` that contains the text of the line of poetry, and a key `gid` that contains the Project Gutenberg ID of the file in question. You can use this ID to look up the title and author of the book of poetry that the line came from (either using the [Project Gutenberg website](https://www.gutenberg.org/) or using pre-built metadata from, e.g., [Gutenberg, dammit](https://github.com/aparrish/gutenberg-dammit/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randompoem = random.sample(all_lines, 8)\n",
    "print(randompoem)\n",
    "print(\"～ ❀ ～\")\n",
    "\n",
    "randompoem_t = [line['s'] for line in randompoem]\n",
    "print(randompoem_t)\n",
    "print(\"～ ❀ ～\")\n",
    "\n",
    "randompoem_lb = \"\\n\".join(randompoem_t)\n",
    "print(randompoem_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "you could also f.ex. find in our random output a specific word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dido = re.search('Dido', randompoem_lb)\n",
    "print(dido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or finding a specific word in the whole document an printing the whole line (or 8 of it) \n",
    "dido_line = [line['s'] for line in all_lines if re.search('Dido', line['s'])]\n",
    "random.sample(dido_line, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
